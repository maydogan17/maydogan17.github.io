<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Murat Han Aydoğan</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta property="og:title" content="Murat Han Aydoğan Resume" />
    <meta property="og:type" content="website" />
    <meta
      property="og:description"
      content="A resume website to showcase my web development skills."
    />
    <meta property="og:image" content="imgs/thumbnail.jpg" />
    <meta property="og:url" content="https://maydogan17.github.io/" />
    <meta name="twitter:card" content="summary_large_image" />

    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-Zenh87qX5JnK2Jl0vWa8Ck2rdkQ2Bzep5IDxbcnCeuOxjzrPF/et3URy9Bv1WTRi"
      crossorigin="anonymous"
    />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.9.1/font/bootstrap-icons.css"
    />
    <link rel="stylesheet" href="css/index.css" />
  </head>
  <body>
    <header class="navbar navbar-expand sticky-bottom" id="home">
        <div class="container-fluid py-3">
          <a class="navbar-brand" href="#home"
            ><i class="bi bi-code h-100"></i
          ></a>
          <div class="navbar-nav">
            <a class="nav-link active" aria-current="page" href="#home">Home</a>
            <a class="nav-link" href="#resume_header">Resume</a>
            <a class="nav-link" href="#">Other</a>
            <a
              class="nav-link"
              href="https://github.com/maydogan17"
              target="_blank"
              ><i class="bi bi-github"></i
            ></a>
            <a
              class="nav-link"
              href="https://www.linkedin.com/in/maydogan17/"
              target="_blank"
              ><i class="bi bi-linkedin"></i
            ></a>
          </div>
        </div>
      </header>
      <div class="container mt-lg-5 mb-lg-5 pb-lg-5 pt-lg-5" id="greeting">
        <div class="row mt-5 mb-5 pb-5 pt-5">
          <div class="col-12 mb-5 pb-5">
            <h4 class="display-6">"Yeess!" "No-oh-oh..." Implicit Robot Task Information from Prosody in Human Verbal Feedback</h4>
          </div>
        </div>
      </div>
      <div class="container-fluid pt-4 pb-4" id="resume_header">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <h2 class="display-6 mb-0">Authors</h2>
            </div>
          </div>
        </div>
      </div>
      <div class="container mt-5 mb-5 pt-5 pb-5" id="content">
        <div class="row">
          <div class="col-sm-4">
            <p class="mb-0">Murat Han Aydoğan</p>
            <p class="mb-0">VU Amsterdam</p>
            <p class="mb-0">maydogan17@ku.edu.tr</p>
            <br>
            <br>
            <p class="mb-0">Taylor Kessler Faulkner</p>
            <p class="mb-0">University of Washington</p>
            <p class="mb-0">taylorkf@uw.edu</p>
          </div>
          <div class="col-sm-4">
            <p class="mb-0">Kenneth D Mitra</p>
            <p class="mb-0">University of Texas at Austin</p>
            <p class="mb-0">kennethmitra@utexas.edu</p>
            <br>
            <br>
            <p class="mb-0">Akanksha Saran</p>
            <p class="mb-0">Microsoft Research NYC</p>
            <p class="mb-0">akanksha.saran@microsoft.com</p>
          </div>
          <div class="col-sm-4">
            <p class="mb-0">Kush Desai</p>
            <p class="mb-0">University of Texas at Austin</p>
            <p class="mb-0">kushkdesai@utexas.edu</p>
            <br>
            <br>
            <p class="mb-0">Kim Baraka</p>
            <p class="mb-0">VU Amsterdam</p>
            <p class="mb-0">k.baraka@vu.nl</p>
          </div>
        </div>
      </div>
      <div class="container-fluid pt-4 pb-4" id="resume_header">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <h2 class="display-6 mb-0">Project</h2>
            </div>
          </div>
        </div>
      </div>
      <div class="container mt-5 mb-5 pt-5 pb-5" id="content">
        <div class="row">
          <div class="col-12 col-lg-12" id="exp">
            <h2 class="display-6 pb-3">Abstract</h2>
            <p class="mb-0">
                This paper presents preliminary evidence that prosody carries useful 
                task information in an interactive reinforcement learning setting with 
                verbal evaluative feedback from a human teacher. We developed a novel 
                mixed-participant Wizard-of-Oz study setup to collect audio data from 
                participants teaching a reinforcement learning agent in a grid world 
                navigation task where the agent was wizarded by another participant, 
                hence simulating prosody-sensitive agent learning. Our pilot study shows 
                that, for the participants tested in the teacher role, prosodic features 
                such as energy and pitch are statistically significantly correlated with 
                the advantage function, an underlying Markov Decision Process feature used 
                by RL algorithms. Results also suggest some level of individual differences 
                between different teachers. While further research is needed to develop 
                computational models of human teachers' prosody in different learning tasks, 
                our early results highlight the potential of tapping into implicit voice 
                signals to improve robot learning and human teaching efficiency.
            </p>
            <h2 class="display-6 pb-3">Introduction</h2>
            <p class="mb-0">
                In order to allow users to have more control over a robot’s
                operation, we believe robots should be able to learn from
                interaction with said users who can take on the role of human
                teachers. One popular approach to address robot learning from
                interaction with human teachers is interactive reinforcement
                learning (intRL), whereby a user provides evaluative feedback
                (e.g., binary positive/negative feedback) to a learning agent, in
                order to influence or shape its policy over time [1]–[3]. This
                setup allows the human teacher to intuitively provide feedback
                on the robot’s actions without requiring demonstrations, which
                are often difficult for non-expert users to provide. Teachers
                also have more flexibility in personalizing the robot’s behavior
                to their own preferences. One popular way for communicating
                this feedback is the use of speech interfaces [4]–[8]. While
                these interfaces are intuitive for untrained users, they mostly
                focus on the content of the speech (the words being uttered)
                and ignore other parts of the voice signals that may be relevant
                for the learner.
                The claim we make in this paper is that prosody, as an
                implicit paralinguistic signal present in human voice, carries
                useful task information. As such, it could be considered
                as a valid teaching signal that complements speech content
                in future intRL research. We conducted a mixed-participant
                Wizard-of-Oz (WoZ) study where, in each session, two par-
                ticipants are secretly matched, one in the role of a teacher
                giving “yes/no” feedback, and one in the role of the Wizard
                controlling the agent through a keyboard. This setup allowed
                us to collect naturalistic teacher data by simulating the learning
                process of the agent using a human Wizard with limited
                task information. We then ran a correlation analysis on the
                teachers’ voice data to relate prosodic features to features of
                the underlying Markov Decision Process (MDP) used to model
                the learning task.
                Our contributions are summarized as follows:
                <ul>
                    <li>
                        A novel mixed-participant WoZ experimental setup to
                        collect naturalistic teacher voice data.
                    </li>
                    <li>
                        A pilot study showing significant correlations between
                        prosodic features of voice data (namely, energy and pitch)
                        with underlying MPD features (namely, the advantage
                        function).
                    </li>
                </ul>
            </p>
            <h2 class="display-6 pb-3">Related Work</h2>
            <p class="mb-0">
                We provide a brief overview of work on learning from
                human audio as well as other implicit social signals.
            </p>
            <b>1. Learning from human audio</b>
            <p class="mb-0">
                Some prior works which leverage human feedback during
                reinforcement learning tasks [9] do so via voice [4]–[8]. Teno-
                rio et al. [4] perform reward shaping using SARSA [10]–[12]
                with human voice. Under their setup, the voice-based feedback
                is provided as the robot is executing the task. However,
                rewards are predefined for certain words in a vocabulary of 250
                words such as +50 for ‘excellent’, -50 for ‘terrible’ etc., and no
                prosodic information is used. Krenig et al. [7] train RL agents
                with action advice in the form of human voice, such that a set
                of predefined words directly map to an underlying action from
                a discrete action set. Krenig et al. [8] use sentiment analysis
                to filter explanations into advice of what to do and warnings
                of what to avoid. Nicolescu et al. [13] demonstrated the role
                of verbal cues both during demonstrations and as feedback
                from the human teacher during the agent’s learning process,
                to facilitate learning of navigation behaviors on a mobile robot,
                with a limited vocabulary of words to indicate relevant parts of
                the workspace or actions that a robot must execute. Similarly,
                Pardowitz et al. [14] used a fixed set of seven vocal comments
                which are mapped one-to-one with features relevant to the
                task to augment subtask similarity detection and learning of
                the task model from demonstrations for a simple table setting
                task. However, all of these prior works focus on the spoken
                words and do not leverage prosody from human speech—an
                informative and rich signal of human intent which has the
                potential to further enhance learning [15].
                Kim et al. [5] use affective human speech feedback over
                25 msec audio snippets to improve a social waving behavior
                using Q learning. They use three prosody features (total band
                energy, variance of log-magnitude-spectrum, variance of log-
                spectral-energy) to learn the wave which optimally satisfies a
                human tutor. We build on this work to further understand how
                prosodic features relate to MDP features, such as the advantage
                function, with the goal to inform the design of future RL
                algorithms which can be more sample efficient by leveraging
                prosodic information.
            </p>
            <b>2. Learning from other implicit signals</b>
            <p class="mb-0">
                The field of robot learning with multi-modal human cues
                [16], [17] has leveraged other implicit signals apart from
                speech, such as clicker-based feedback (perfect and imper-
                fect) [18]–[20], eye movements [21]–[24], facial expres-
                sions [25], [26], gestures [26], [27], haptic feedback [28], [29],
                object and environmental sounds [30]–[33]. Verbal prosodic
                feedback is a relatively under-explored modality for intRL.
                However, there has been some recent advances which highlight
                the richness of prosody for robot learning from demonstra-
                tion [15]. In our work, we build on this work to further develop
                an understanding of human prosodic features for intRL.
            </p>
            <h2 class="display-6 pb-3">Mixed-Participant Wizard-of-Oz Setup</h2>
            <p class="mb-0">
                In order to develop an algorithm that leverages implicit
                information in prosody, we need to first understand how people
                use prosody as a teaching signal. On the other hand, in order
                to understand prosodic behavior in this context, we need to
                have an algorithm that incorporates prosody in its learning,
                which we don’t have yet. To solve this paradox, we opted
                for a mixed participant Wizard-of-Oz approach where one
                participant plays the role of a teacher, and the other participant
                with no information about the task plays the role of a Wizard.
                This setup makes sure that the teacher audio we get is as close
                a possible to what we would expect in our target context. This
                approach is superior to using a baseline algorithm for the agent
                (e.g., intRL based on speech only) for two reasons. First, we
                expect the teacher to adapt to the learner, thereby potentially
                suppressing their prosodic signals (which was confirmed in
                some early pilots we ran with fixed agent trajectories). Second,
                the wizard’s keystrokes provide us with valuable data that can
                be used in future research to better understand local and global
                interpretations of teacher feedback, independently of how well
                the teacher is able to teach.
                Our contributed web-based WoZ interface is shown in
                Fig. 1. While the teacher sees the full environment and
                provides online verbal feedback to the agent in real-time,
                the Wizard receives the teacher audio in real-time (streamed
                through Twilio, a secure web service) and needs to control
                the agent through keystrokes in response to current and past
                feedback from the teacher. The Wizard is shown a sanitized
                view of the environment that only shows the grid. The two
                environments are synchronized over web sockets to ensure
                consistent agent positions on both interfaces. The code will
                be released at [url-redacted] to facilitate further research in
                the interactive learning community.
            </p>
            <h2 class="display-6 pb-3">Study Design</h2>
            <b>1. Participants</b>
            <p class="mb-0">
                We recruited eight university students as participants for our
                pilot study, four of which were assigned the role of teachers
                and four of them the role of wizard (randomly), with English
                fluency as an inclusion criterion. In the teacher role, half the
                participants identified as female and the other half identified
                as male. Three of them were native English speakers. They
                all have experience with playing video games and three of
                them have experience with programming. On the wizard side,
                three identified themselves as male and one as female. Two
                of them were native English speakers. All of the wizards have
                experience with programming and playing video games.
            </p>
            <br>
            <b>2. Experiment Setup</b>
            <p class="mb-0">
                Figure 1 shows what each participant saw on the screen. The
                teacher sees the whole map of the game with full details while
                the wizard only sees the robot and the map. The Wizard had no
                previous idea about what the task entailed or where the special
                states were located. The map was created with wall borders
                around the playable area. The robot location was initialized
                at the start of the game with a uniform random selection.
                The robot’s starting direction was picked randomly from
                four options: up, down, left, and right. Also, the robot was
                initialized in such a way that it could move three spaces in its
                starting direction without hitting a wall. Game elements were
                placed last with the following constraints: bombs must not
                be within 3 spaces of the robot’s initial direction of travel and
                the Manhattan distance between elements must be 4 or greater.
                Unknown to the teacher, the wizard controlled the robot with
                arrow keys on the keyboard and in the absence of wizard input
                for 1.25 seconds or more, the robot started exploring the map
                randomly, mimicking exploration/exploitation phases of most
                RL algorithms. The game concluded after one practice round
                (until the goal was reached) and three actual game rounds used
                for analysis.
            </p>
            <br>
            <br>
            <div class="text-center">
                <img src="imgs/fig1.png" class="rounded" alt="">
            </div>
            <div class="text-center">
                <p class="mb-0">Fig. 1. Mixed Participant Wizard of Oz setup.</p>
            </div>
            <br>
            <b>3. Procedure</b>
            <p class="mb-0">
                Pairs of participants were appointed simultaneously for the
                study and welcomed by the examiner separately in either the
                teacher or the wizard role. All participants were presented with
                a relevant consent form prior to the session, with options for
                the teacher to choose how their data is shared according to
                privacy preferences. The study was approved by the university
                ethical committees of two of our affiliations.</p>
            <p class="mb-0">A) Teacher: As a baseline recording of the teacher’s voice
                prior to the start of the experimental part, the participant
                read a small paragraph given to them, which takes about 30
                seconds to read. During the actual experiment, the teacher
                was only allowed to use the words “yes” and “no”. To
                elicit richer prosodic variations, we instructed the teachers
                to speak as if they would with a 2 year old child. Before
                the experiment, the teacher was told that the agent would be
                listening to their voice, including “how” they spoke, and acting
                accordingly. In reality, the another participant in the role of
                Wizard was controlling the agent in response to the teachers’
                verbal feedback. The teacher was briefed about this deception
                after the experiment and was asked not to talk about this with
                other possible participants.</p>
            <p class="mb-0">B) Wizard: The wizard was briefed on how to play the
                game. Their keyboard interactions were recorded alongside
                other details about the game such as: immediate rewards,
                timestamps, movements of the agent, and score. After the
                experiment, the wizard was asked not to talk about the
                experiment procedure with other potential participants.
            </p>
            <br>
            <b>4. Measures</b>
            <p class="mb-0">
                Given the scope of the paper, we only focus on measures
                of the teacher’s data. Analysis of the Wizard data is left for
                future work.
            </p>
            <p class="mb-0">
                A) Prosodic features: We computed several acoustic fea-
                tures over the detected utterances to characterize prosody from
                human verbal feedback. We studied three different features
                to capture prosody and affect – energy, loudness, and pitch.
                These acoustic features have been shown to enhance seman-
                tic parsing [34], understand speech recognition failures in
                dialogue systems [35], and widely used for applications in
                human-robot interaction [6], [36] and speech recognition [37]
                communities. These features were extracted from the audio
                recording of the experiment sessions. First, transcriptions of
                the recordings were created using Google Cloud’s Speech-To-
                Text [38]. The transcriptions were used to filter out silent parts
                and speech other than “yes” and “no” (which mostly consisted
                of non-verbal sounds). After correctly identifying where the
                relevant speech took place, prosody features were calculated.
                Librosa [39] was used for calculating the loudness and energy
                of the audio. Pitch (or frequency) was analyzed using a pre-
                trained model for a data-driven pitch tracking algorithm [40].
                During the analysis, mean, max, and raw values were used as
                candidate measures.
            </p>
          </div>
        </div>
      </div>

      <button
      type="button"
      class="btn btn-secondary btn-floating text-center"
      id="btn-back-to-top"
    >
      <i class="bi bi-arrow-up"></i>
    </button>
    <footer class="pt-5 pb-5">
      <div class="container">
        <div class="row">
          <div class="col-lg-3 col-12 text-center">
            <p>
              <i class="bi bi-envelope"></i>
              <a href="mailto: maydogan17@ku.edu.tr" target="_blank"
                >maydogan17@ku.edu.tr</a
              >
            </p>
          </div>
          <div class="col-lg-3 col-12 text-center">
            <p><i class="bi bi-telephone"></i> +90 505 260 64 17</p>
          </div>
          <div class="col-lg-3 col-12 text-center">
            <address>Rumeli Feneri Yolu, Sarıyer, İstanbul</address>
          </div>
          <div class="col-lg-3 col-12 text-center">
            <a href="https://twitter.com/murathanaydgn" target="_blank"
              ><i class="bi bi-twitter"></i> @murathanaydgn</a
            >
          </div>
        </div>
      </div>
    </footer>
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-OERcA2EqjJCMA+/3y+gxIOqMEjwtxJY7qPCqsdltbNJuaOe923+mo//f6V8Qbsw3"
      crossorigin="anonymous"
    ></script>
    <script type="text/javascript" src="js/index.js"></script>
  </body>
</html>